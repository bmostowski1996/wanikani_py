{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac19ac1",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5d9bf",
   "metadata": {},
   "source": [
    "The goal of this code is to use the Wanikani API to retrieve review items that have not been \"burned\" by the user despite availability for a long time so that the user can further review those items. The end output of this code is two .csv files that can be imported into Anki to be used as flash deck cards for further study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adaea418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import requests\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "token = 'YOUR TOKEN HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf813de",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "618cbcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbg_print(statement, cond = True):\n",
    "    if cond == True:\n",
    "        print(statement)\n",
    "        \n",
    "debug_cond = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38032b32",
   "metadata": {},
   "source": [
    "The definition of wanikani_req is due to trunklayer. (See https://community.wanikani.com/t/python-api-call-headers-problem/45614/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a04eb559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wanikani_req(token, endpoint):\n",
    "    \n",
    "    address = 'https://api.wanikani.com/v2'\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {token}'\n",
    "    }\n",
    "\n",
    "    with requests.get(f'{address}/{endpoint}', headers=headers) as r:\n",
    "        response = r.json()\n",
    "    #pprint(response)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e5567f",
   "metadata": {},
   "source": [
    "If a request from the wanikani API is too big, it is split into multiple pages. The following code ensures that all requests are pulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e23382d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wanikani_pull(token, endpoint, debug_cond = False):\n",
    "    #ACQUIRING THE RESOURCES\n",
    "    #First, let's get a list of all the requests. Unfortunately, we are limited to 500 resources returned for collection endpoints\n",
    "    #This means if there's more than 500 resources we need to return, we need to navigate Wanikani's pagination system.\n",
    "    #(EXCEPTION TO THE ABOVE, some API calls return 1000 resources at a time.)\n",
    "    \n",
    "    orig_endpoint = endpoint\n",
    "    dbg_print(\"TOKEN SPECIFIED: \" + token, debug_cond)\n",
    "    dbg_print(\"ENDPOINT SPECIFIED: \" + orig_endpoint, debug_cond)\n",
    "    \n",
    "    assignment_arr = []\n",
    "\n",
    "    req = wanikani_req(token, endpoint)\n",
    "    requests_to_get = req['total_count']\n",
    "\n",
    "    dbg_print(\"TOTAL REQUESTS TO GET: \" + str(requests_to_get), debug_cond)\n",
    "\n",
    "    assignment_arr = assignment_arr + req['data']\n",
    "    requests_to_get = requests_to_get - len(req['data'])\n",
    "\n",
    "    dbg_print(\"After batch 1 added...\", debug_cond)\n",
    "    dbg_print(\"TOTAL REQUESTS TO GET: \" + str(requests_to_get), debug_cond)\n",
    "\n",
    "    i = 1\n",
    "    while requests_to_get > 0:\n",
    "        i = i + 1\n",
    "        \n",
    "        #Get the info from the JSON that we need to pull the next page\n",
    "        url_info = str(req['pages']['next_url'])\n",
    "        address = 'https://api.wanikani.com/v2'\n",
    "        endpoint = url_info[len(address)+1:]\n",
    "        \n",
    "        #Now, make a request with the new endpoint\n",
    "        dbg_print(\"NEW ENDPOINT: \" + endpoint, debug_cond)\n",
    "        req = wanikani_req(token, endpoint)\n",
    "        assignment_arr = assignment_arr + req['data']\n",
    "        requests_to_get = requests_to_get - len(req['data'])\n",
    "\n",
    "        dbg_print(\"After batch \" + str(i) + \" added... \", debug_cond)\n",
    "        dbg_print(\"TOTAL REQUESTS TO GET: \" + str(requests_to_get), debug_cond)\n",
    "\n",
    "    #This final step is so that our data appears in a \"JSON-like\" format\n",
    "    assignment_dict = {\"data\": assignment_arr}\n",
    "    dbg_print(\"PULL COMPLETED!\", debug_cond)\n",
    "    return assignment_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3912bb32",
   "metadata": {},
   "source": [
    "Before I can pass the dates as date variables into the SQL database, I need to clean them up to meet SQL's datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88200165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_cleanup(date):\n",
    "    #The argument will take the form of something like '2021-09-29T00:27:27.473588Z'\n",
    "    try:\n",
    "        SQL_date = date[0:10] + ' ' + date[11:11+8]\n",
    "    except:\n",
    "        SQL_date = \"NULL\"\n",
    "    \n",
    "    return SQL_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9309b4a3",
   "metadata": {},
   "source": [
    "Useful helper function for computing the date and time of some days ago (and returning it as a string that can be used in SQL queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8d217ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_days_ago(days):\n",
    "    days = days\n",
    "\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    cutoff_line = current_datetime - datetime.timedelta(days = days)\n",
    "\n",
    "    date_str = cutoff_line.strftime(\"%Y-%m-%d\")\n",
    "    time_str = cutoff_line.strftime(\"%H:%M:%S\")\n",
    "\n",
    "    cutoff_date =  date_str + \" \" + time_str\n",
    "    \n",
    "    return cutoff_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0e485",
   "metadata": {},
   "source": [
    "In case I need to clean the slate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6209f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there's already a database file with the name we want, let's trash it first before doing anything:\n",
    "def sqlite_delete(db_path):\n",
    "    db_path = db_path\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(db_path):\n",
    "        # Delete the file\n",
    "        os.remove(db_path)\n",
    "        print(f'Database {db_path} deleted.')\n",
    "    else:\n",
    "        print(f'Database {db_path} does not exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6e80401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite_schema(db_path, schema):\n",
    "    # create an SQLite schema based on JSON-like data\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    conn.execute(schema)\n",
    "    print('Schema at path ' + db_path + ' successfully executed')\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a718a",
   "metadata": {},
   "source": [
    "# Building a Database of Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c70effe",
   "metadata": {},
   "source": [
    "Next, let's create the \"schema\", or structure for the SQL database that we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2146ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database wanikani.sqlite deleted.\n",
      "Schema at path wanikani.sqlite successfully executed\n",
      "Schema at path wanikani.sqlite successfully executed\n"
     ]
    }
   ],
   "source": [
    "# create an SQL schema based on the JSON-like data\n",
    "db_path = 'wanikani.sqlite'\n",
    "sqlite_delete(db_path)\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "conn.execute(\"DROP TABLE IF EXISTS assignments\")\n",
    "conn.execute(\"DROP TABLE IF EXISTS subjects\")\n",
    "\n",
    "schema_assignments = '''\n",
    "CREATE TABLE assignments (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    subject_id INTEGER,\n",
    "    subject_type TEXT,\n",
    "    srs_stage INTEGER,\n",
    "    unlocked_at DATETIME,\n",
    "    started_at DATETIME,\n",
    "    passed_at DATETIME,\n",
    "    burned_at DATETIME,\n",
    "    available_at DATETIME,\n",
    "    resurrected_at DATETIME,\n",
    "    hidden BOOL\n",
    ");\n",
    "'''\n",
    "\n",
    "schema_subjects = '''\n",
    "CREATE TABLE subjects (\n",
    "    subject_id INTEGER PRIMARY KEY,\n",
    "    level INTEGER,\n",
    "    type TEXT,\n",
    "    slug TEXT,\n",
    "    reading TEXT,\n",
    "    meaning TEXT\n",
    ");\n",
    "'''\n",
    "\n",
    "sqlite_schema(db_path, schema_assignments)\n",
    "sqlite_schema(db_path, schema_subjects)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c10db74",
   "metadata": {},
   "source": [
    "Finally, let's build the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "afb7c988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHANGES COMMITTED TO DATABASE!\n",
      "CHANGES COMMITTED TO DATABASE!\n"
     ]
    }
   ],
   "source": [
    "endpoint = 'assignments?started=true'\n",
    "assignment_dict = wanikani_pull(token, endpoint)\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "# insert the data into the database\n",
    "for obj in assignment_dict['data']:\n",
    "    #Note, the dates need to all be cleaned up before I can pass\n",
    "    val_1 = obj['id']\n",
    "    val_2 = obj['data']['subject_id']\n",
    "    val_3 = obj['data']['subject_type']\n",
    "    val_4 = obj['data']['srs_stage']\n",
    "    val_5 = date_cleanup(obj['data']['unlocked_at'])\n",
    "    val_6 = date_cleanup(obj['data']['started_at'])\n",
    "    val_7 = date_cleanup(obj['data']['passed_at'])\n",
    "    val_8 = date_cleanup(obj['data']['burned_at'])\n",
    "    val_9 = date_cleanup(obj['data']['available_at'])\n",
    "    val_10 = date_cleanup(obj['data']['resurrected_at'])\n",
    "    val_11 = obj['data']['hidden']\n",
    "    \n",
    "    conn.execute(\"INSERT INTO assignments (id, subject_id, subject_type, srs_stage, unlocked_at, started_at, passed_at, burned_at, available_at, resurrected_at, hidden) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\", (val_1, val_2, val_3, val_4, val_5, val_6, val_7, val_8, val_9, val_10, val_11))\n",
    "\n",
    "# commit the changes to the database\n",
    "conn.commit()\n",
    "print(\"CHANGES COMMITTED TO DATABASE!\")\n",
    "\n",
    "endpoint = 'subjects?types=kanji,vocabulary'\n",
    "assignment_dict = wanikani_pull(token, endpoint)\n",
    "\n",
    "conn = sqlite3.connect('wanikani.sqlite')\n",
    "# insert the data into the database\n",
    "for obj in assignment_dict['data']:\n",
    "    #Note, the dates need to all be cleaned up before I can pass\n",
    "    val_1 = obj['id']\n",
    "    val_2 = obj['data']['level']\n",
    "    val_3 = obj['object']\n",
    "    val_4 = obj['data']['slug']\n",
    "    val_5 = obj['data']['readings'][0]['reading']\n",
    "    val_6 = obj['data']['meanings'][0]['meaning']\n",
    "    \n",
    "    conn.execute(\"INSERT INTO subjects (subject_id, level, type, slug, reading, meaning) VALUES (?, ?, ?, ?, ?, ?)\", (val_1, val_2, val_3, val_4, val_5, val_6))\n",
    "\n",
    "# commit the changes to the database\n",
    "conn.commit()\n",
    "print(\"CHANGES COMMITTED TO DATABASE!\")\n",
    "\n",
    "# close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d214bf30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              item reading\n",
      "0      右 (reading)      ゆう\n",
      "1      石 (reading)      せき\n",
      "2      立 (reading)      りつ\n",
      "3      友 (reading)      ゆう\n",
      "4      少 (reading)     しょう\n",
      "5      心 (reading)      しん\n",
      "6      写 (reading)      しゃ\n",
      "7      土 (reading)      つち\n",
      "8     六日 (reading)     むいか\n",
      "9     王子 (reading)     おうじ\n",
      "10    戸口 (reading)     とぐち\n",
      "11     方 (reading)      かた\n",
      "12     北 (reading)      きた\n",
      "13    中古 (reading)    ちゅうこ\n",
      "14    広い (reading)     ひろい\n",
      "15    公用 (reading)    こうよう\n",
      "16    先ず (reading)      まず\n",
      "17    先日 (reading)    せんじつ\n",
      "18    名人 (reading)    めいじん\n",
      "19   一文字 (reading)   いちもんじ\n",
      "20    元気 (reading)     げんき\n",
      "21    人口 (reading)    じんこう\n",
      "22    七日 (reading)     なのか\n",
      "23   丸ごと (reading)    まるごと\n",
      "24  口にする (reading)   くちにする\n",
      "              item              meaning\n",
      "0      右 (meaning)                Right\n",
      "1      石 (meaning)                Stone\n",
      "2      立 (meaning)                Stand\n",
      "3      友 (meaning)               Friend\n",
      "4      少 (meaning)                  Few\n",
      "5      心 (meaning)                Heart\n",
      "6      写 (meaning)                 Copy\n",
      "7      土 (meaning)                 Soil\n",
      "8     六日 (meaning)            Sixth Day\n",
      "9     王子 (meaning)               Prince\n",
      "10    戸口 (meaning)              Doorway\n",
      "11     方 (meaning)                  Way\n",
      "12     北 (meaning)                North\n",
      "13    中古 (meaning)           Secondhand\n",
      "14    広い (meaning)                 Wide\n",
      "15    公用 (meaning)  Government Business\n",
      "16    先ず (meaning)         First Of All\n",
      "17    先日 (meaning)        The Other Day\n",
      "18    名人 (meaning)               Expert\n",
      "19   一文字 (meaning)        Straight Line\n",
      "20    元気 (meaning)               Energy\n",
      "21    人口 (meaning)           Population\n",
      "22    七日 (meaning)          Seventh Day\n",
      "23   丸ごと (meaning)      In Its Entirety\n",
      "24  口にする (meaning)           To Mention\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(db_path)\n",
    "conn.execute(\"DROP TABLE IF EXISTS main\")\n",
    "\n",
    "query =  f\"CREATE TABLE main AS SELECT * FROM subjects INNER JOIN assignments ON subjects.subject_id = assignments.subject_id\"\n",
    "conn.execute(query)\n",
    "\n",
    "cutoff_date = x_days_ago(360)\n",
    "\n",
    "query_table = pd.read_sql_query(f\"SELECT slug || ' (reading)' as item, reading FROM main WHERE burned_at = 'NULL' AND started_at < '{cutoff_date}' ORDER BY subject_id ASC;\", conn)\n",
    "query_table.to_csv('readings_to_practice.csv', index=False)\n",
    "pprint(query_table)\n",
    "\n",
    "query_table = pd.read_sql_query(f\"SELECT slug || ' (meaning)' as item, meaning FROM main WHERE burned_at = 'NULL' AND started_at < '{cutoff_date}' ORDER BY subject_id ASC;\", conn)\n",
    "query_table.to_csv('meanings_to_practice.csv', index=False)\n",
    "pprint(query_table)\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
